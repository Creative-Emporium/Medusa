{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.14s/it]\n",
      "Some weights of MedusaModelLlama were not initialized from the model checkpoint at /models/Meta-Llama-3-8B-Instruct and are newly initialized: ['medusa_head.1.0.linear.weight', 'medusa_head.0.0.linear.bias', 'medusa_head.3.1.weight', 'medusa_head.3.0.linear.bias', 'medusa_head.0.1.weight', 'medusa_head.0.0.linear.weight', 'medusa_head.2.0.linear.weight', 'medusa_head.1.1.weight', 'medusa_head.3.0.linear.weight', 'medusa_head.2.0.linear.bias', 'medusa_head.4.0.linear.weight', 'medusa_head.2.1.weight', 'medusa_head.1.0.linear.bias', 'medusa_head.4.0.linear.bias', 'medusa_head.4.1.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MedusaModelLlama(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       "  (medusa_head): ModuleList(\n",
       "    (0-4): 5 x Sequential(\n",
       "      (0): ResBlock(\n",
       "        (linear): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "        (act): SiLU()\n",
       "      )\n",
       "      (1): Linear(in_features=4096, out_features=128256, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from medusa.model.medusa_model import MedusaModel\n",
    "\n",
    "\n",
    "model_path = '/models/train/0626_medusa_mlp_Meta-Llama-3-8B-Instruct_medusa_3_lr_0.0001_layers_1'\n",
    "\n",
    "\n",
    "model = MedusaModel.from_pretrained(\n",
    "\tmodel_path,\n",
    "\ttorch_dtype=torch.float32,\n",
    "\tlow_cpu_mem_usage=True,\n",
    "\tdevice_map='cpu',\n",
    ")\n",
    "model.eval()\n",
    "tokenizer = model.get_tokenizer()\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'Give a name of a color, answer in one word.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nSay a word, answer in one word.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fastchat.model.model_adapter import get_conversation_template\n",
    "\n",
    "\n",
    "conv = get_conversation_template(model_path)\n",
    "conv.append_message(conv.roles[0], query)\n",
    "conv.append_message(conv.roles[1], None)\n",
    "prompt = conv.get_prompt()\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[128000, 128000, 128006,    882, 128007,    271,  46864,    264,   3492,\n",
       "             11,   4320,    304,    832,   3492,     13, 128009, 128006,  78191,\n",
       "         128007,    271]])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from medusa.model.kv_cache import initialize_past_key_values\n",
    "from medusa.model.utils import reset_medusa_mode\n",
    "\n",
    "\n",
    "(\n",
    "\tpast_key_values,\n",
    "\tpast_key_values_data,\n",
    "\tcurrent_length_data,\n",
    ") = initialize_past_key_values(model.base_model)\n",
    "model.past_key_values = past_key_values\n",
    "model.past_key_values_data = past_key_values_data\n",
    "model.current_length_data = current_length_data\n",
    "\n",
    "reset_medusa_mode(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  4.8914,   6.0422,  10.7782,  ...,  -3.6068,  -3.6067,  -3.6066],\n",
       "         [  4.8914,   6.0422,  10.7782,  ...,  -3.6068,  -3.6067,  -3.6066],\n",
       "         [ -2.6605,   1.5771,  -2.3671,  ...,   6.2346,   6.2343,   6.2343],\n",
       "         ...,\n",
       "         [-15.8961, -12.5432, -12.6700,  ...,  11.0865,  11.0863,  11.0865],\n",
       "         [ -0.9013,   1.4144,  -4.5220,  ...,   4.8689,   4.8691,   4.8691],\n",
       "         [  7.2457,  17.8507,   4.8114,  ...,  -0.0681,  -0.0680,  -0.0680]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = model(input_ids, past_key_values=model.past_key_values, output_orig=True, medusa_forward=False).logits\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.4926e-06, 6.0201e-02, 1.3084e-07,  ..., 9.9444e-10, 9.9453e-10,\n",
       "         9.9453e-10]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_p = torch.softmax(logits[:, -1], dim=-1)\n",
    "sample_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128256])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([[0.6802, 0.0654, 0.0602, 0.0467, 0.0160, 0.0137, 0.0092, 0.0058, 0.0056,\n",
       "         0.0054]], grad_fn=<TopkBackward0>),\n",
       "indices=tensor([[ 9906, 27665,     1, 16440,  3968, 38432, 77119, 50040,  6670, 68583]]))"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p10 = torch.topk(sample_p, 10, dim=-1)\n",
    "p10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HelloApple\"CloudFlComputerMoonDogTreeSpark'"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(p10.indices.view(-1), skip_special_tokens=True, spaces_between_special_tokens=False, clean_up_tokenization_spaces=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Apple',\n",
       " '\"',\n",
       " 'Cloud',\n",
       " 'Fl',\n",
       " 'Computer',\n",
       " 'Moon',\n",
       " 'Dog',\n",
       " 'Tree',\n",
       " 'Spark']"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tokenizer.decode([id]) for id in p10.indices.view(-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{9906: 6753,\n",
       " 1: 616,\n",
       " 16440: 462,\n",
       " 27665: 631,\n",
       " 61570: 1,\n",
       " 11839: 4,\n",
       " 3968: 167,\n",
       " 6670: 68,\n",
       " 38432: 126,\n",
       " 77119: 96,\n",
       " 25099: 5,\n",
       " 43069: 42,\n",
       " 62190: 24,\n",
       " 10713: 11,\n",
       " 50040: 64,\n",
       " 44: 42,\n",
       " 31998: 16,\n",
       " 34: 45,\n",
       " 47416: 23,\n",
       " 33413: 11,\n",
       " 22691: 12,\n",
       " 1966: 1,\n",
       " 37: 49,\n",
       " 94357: 1,\n",
       " 42737: 14,\n",
       " 97824: 3,\n",
       " 8325: 2,\n",
       " 74627: 70,\n",
       " 29707: 26,\n",
       " 68781: 10,\n",
       " 10370: 1,\n",
       " 13347: 30,\n",
       " 15339: 23,\n",
       " 10115: 9,\n",
       " 30233: 2,\n",
       " 49540: 1,\n",
       " 10902: 24,\n",
       " 25821: 1,\n",
       " 33813: 30,\n",
       " 51812: 1,\n",
       " 91056: 1,\n",
       " 68583: 50,\n",
       " 11116: 2,\n",
       " 112584: 1,\n",
       " 27899: 33,\n",
       " 31955: 12,\n",
       " 31192: 2,\n",
       " 83380: 7,\n",
       " 97283: 20,\n",
       " 23182: 7,\n",
       " 111491: 1,\n",
       " 33947: 23,\n",
       " 95570: 1,\n",
       " 24581: 1,\n",
       " 75613: 4,\n",
       " 30197: 1,\n",
       " 62816: 3,\n",
       " 6219: 4,\n",
       " 24818: 4,\n",
       " 19753: 12,\n",
       " 29353: 3,\n",
       " 77610: 2,\n",
       " 62528: 3,\n",
       " 79178: 1,\n",
       " 25025: 1,\n",
       " 47: 13,\n",
       " 33274: 7,\n",
       " 330: 1,\n",
       " 25173: 1,\n",
       " 119581: 1,\n",
       " 36152: 1,\n",
       " 103724: 1,\n",
       " 1671: 3,\n",
       " 29296: 4,\n",
       " 9642: 4,\n",
       " 14588: 1,\n",
       " 51787: 9,\n",
       " 31380: 1,\n",
       " 2118: 2,\n",
       " 25310: 3,\n",
       " 55471: 1,\n",
       " 87703: 10,\n",
       " 46240: 1,\n",
       " 17863: 6,\n",
       " 80871: 2,\n",
       " 7280: 14,\n",
       " 81117: 4,\n",
       " 60139: 4,\n",
       " 44638: 2,\n",
       " 8586: 7,\n",
       " 17111: 1,\n",
       " 30635: 1,\n",
       " 20357: 1,\n",
       " 51341: 6,\n",
       " 91963: 3,\n",
       " 125543: 1,\n",
       " 74730: 1,\n",
       " 42280: 1,\n",
       " 45443: 4,\n",
       " 33: 2,\n",
       " 61161: 1,\n",
       " 87655: 6,\n",
       " 16537: 1,\n",
       " 10343: 1,\n",
       " 17911: 1,\n",
       " 17344: 1,\n",
       " 37007: 2,\n",
       " 29351: 7,\n",
       " 4525: 1,\n",
       " 12988: 3,\n",
       " 3617: 2,\n",
       " 86799: 2,\n",
       " 70621: 2,\n",
       " 5191: 1,\n",
       " 12331: 5,\n",
       " 40249: 1,\n",
       " 45202: 1,\n",
       " 81244: 1,\n",
       " 58841: 1,\n",
       " 9028: 3,\n",
       " 23958: 1,\n",
       " 11087: 3,\n",
       " 76: 1,\n",
       " 3103: 1,\n",
       " 58922: 3,\n",
       " 22294: 1,\n",
       " 54: 1,\n",
       " 52782: 1,\n",
       " 24748: 3,\n",
       " 24374: 1,\n",
       " 41: 1,\n",
       " 34791: 1,\n",
       " 32: 1,\n",
       " 27814: 1,\n",
       " 7293: 1,\n",
       " 15724: 1,\n",
       " 61067: 1,\n",
       " 63385: 1,\n",
       " 43: 1,\n",
       " 65641: 3,\n",
       " 1305: 1,\n",
       " 32561: 1,\n",
       " 43211: 1,\n",
       " 80849: 2,\n",
       " 87373: 1,\n",
       " 3513: 4,\n",
       " 26208: 3,\n",
       " 123821: 1,\n",
       " 46439: 2,\n",
       " 85743: 2,\n",
       " 46639: 1,\n",
       " 89825: 1,\n",
       " 32641: 1,\n",
       " 7457: 1,\n",
       " 48799: 3,\n",
       " 80171: 2,\n",
       " 14235: 1,\n",
       " 82681: 2,\n",
       " 95171: 1,\n",
       " 98377: 1,\n",
       " 42398: 1,\n",
       " 96129: 1,\n",
       " 61749: 1,\n",
       " 85103: 1,\n",
       " 25303: 1,\n",
       " 73256: 1,\n",
       " 62783: 1,\n",
       " 76924: 1,\n",
       " 35079: 1,\n",
       " 13820: 1,\n",
       " 31916: 2,\n",
       " 30081: 1,\n",
       " 48414: 1,\n",
       " 8538: 1,\n",
       " 16856: 1,\n",
       " 77850: 1,\n",
       " 47048: 1,\n",
       " 62919: 1,\n",
       " 35919: 1,\n",
       " 23662: 1,\n",
       " 50742: 1,\n",
       " 36966: 1,\n",
       " 38: 1,\n",
       " 60860: 1,\n",
       " 27987: 1,\n",
       " 78286: 1,\n",
       " 9528: 1,\n",
       " 10544: 1,\n",
       " 12978: 1,\n",
       " 50: 1,\n",
       " 51: 1,\n",
       " 9052: 1,\n",
       " 50872: 1,\n",
       " 27014: 1,\n",
       " 58524: 1,\n",
       " 97014: 1,\n",
       " 26004: 1,\n",
       " 51757: 1,\n",
       " 2059: 2,\n",
       " 19410: 1,\n",
       " 26509: 1,\n",
       " 80196: 1,\n",
       " 15881: 1,\n",
       " 69112: 1,\n",
       " 25797: 1,\n",
       " 76094: 1,\n",
       " 44211: 1,\n",
       " 56753: 1,\n",
       " 46674: 1,\n",
       " 6777: 1,\n",
       " 15772: 1,\n",
       " 56436: 1,\n",
       " 7778: 1,\n",
       " 70087: 1,\n",
       " 80659: 1}"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = {}\n",
    "\n",
    "for i in range(100000):\n",
    "    id = torch.multinomial(sample_p, 1).item()\n",
    "    if id not in count:\n",
    "        count[id] = 0\n",
    "    count[id] += 1\n",
    "\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(9906, 6753),\n",
       " (27665, 631),\n",
       " (1, 616),\n",
       " (16440, 462),\n",
       " (3968, 167),\n",
       " (38432, 126),\n",
       " (77119, 96),\n",
       " (74627, 70),\n",
       " (6670, 68),\n",
       " (50040, 64),\n",
       " (68583, 50),\n",
       " (37, 49),\n",
       " (34, 45),\n",
       " (43069, 42),\n",
       " (44, 42),\n",
       " (27899, 33),\n",
       " (13347, 30),\n",
       " (33813, 30),\n",
       " (29707, 26),\n",
       " (62190, 24),\n",
       " (10902, 24),\n",
       " (47416, 23),\n",
       " (15339, 23),\n",
       " (33947, 23),\n",
       " (97283, 20),\n",
       " (31998, 16),\n",
       " (42737, 14),\n",
       " (7280, 14),\n",
       " (47, 13),\n",
       " (22691, 12),\n",
       " (31955, 12),\n",
       " (19753, 12),\n",
       " (10713, 11),\n",
       " (33413, 11),\n",
       " (68781, 10),\n",
       " (87703, 10),\n",
       " (10115, 9),\n",
       " (51787, 9),\n",
       " (83380, 7),\n",
       " (23182, 7),\n",
       " (33274, 7),\n",
       " (8586, 7),\n",
       " (29351, 7),\n",
       " (17863, 6),\n",
       " (51341, 6),\n",
       " (87655, 6),\n",
       " (25099, 5),\n",
       " (12331, 5),\n",
       " (11839, 4),\n",
       " (75613, 4),\n",
       " (6219, 4),\n",
       " (24818, 4),\n",
       " (29296, 4),\n",
       " (9642, 4),\n",
       " (81117, 4),\n",
       " (60139, 4),\n",
       " (45443, 4),\n",
       " (3513, 4),\n",
       " (97824, 3),\n",
       " (62816, 3),\n",
       " (29353, 3),\n",
       " (62528, 3),\n",
       " (1671, 3),\n",
       " (25310, 3),\n",
       " (91963, 3),\n",
       " (12988, 3),\n",
       " (9028, 3),\n",
       " (11087, 3),\n",
       " (58922, 3),\n",
       " (24748, 3),\n",
       " (65641, 3),\n",
       " (26208, 3),\n",
       " (48799, 3),\n",
       " (8325, 2),\n",
       " (30233, 2),\n",
       " (11116, 2),\n",
       " (31192, 2),\n",
       " (77610, 2),\n",
       " (2118, 2),\n",
       " (80871, 2),\n",
       " (44638, 2),\n",
       " (33, 2),\n",
       " (37007, 2),\n",
       " (3617, 2),\n",
       " (86799, 2),\n",
       " (70621, 2),\n",
       " (80849, 2),\n",
       " (46439, 2),\n",
       " (85743, 2),\n",
       " (80171, 2),\n",
       " (82681, 2),\n",
       " (31916, 2),\n",
       " (2059, 2),\n",
       " (61570, 1),\n",
       " (1966, 1),\n",
       " (94357, 1),\n",
       " (10370, 1),\n",
       " (49540, 1),\n",
       " (25821, 1),\n",
       " (51812, 1),\n",
       " (91056, 1),\n",
       " (112584, 1),\n",
       " (111491, 1),\n",
       " (95570, 1),\n",
       " (24581, 1),\n",
       " (30197, 1),\n",
       " (79178, 1),\n",
       " (25025, 1),\n",
       " (330, 1),\n",
       " (25173, 1),\n",
       " (119581, 1),\n",
       " (36152, 1),\n",
       " (103724, 1),\n",
       " (14588, 1),\n",
       " (31380, 1),\n",
       " (55471, 1),\n",
       " (46240, 1),\n",
       " (17111, 1),\n",
       " (30635, 1),\n",
       " (20357, 1),\n",
       " (125543, 1),\n",
       " (74730, 1),\n",
       " (42280, 1),\n",
       " (61161, 1),\n",
       " (16537, 1),\n",
       " (10343, 1),\n",
       " (17911, 1),\n",
       " (17344, 1),\n",
       " (4525, 1),\n",
       " (5191, 1),\n",
       " (40249, 1),\n",
       " (45202, 1),\n",
       " (81244, 1),\n",
       " (58841, 1),\n",
       " (23958, 1),\n",
       " (76, 1),\n",
       " (3103, 1),\n",
       " (22294, 1),\n",
       " (54, 1),\n",
       " (52782, 1),\n",
       " (24374, 1),\n",
       " (41, 1),\n",
       " (34791, 1),\n",
       " (32, 1),\n",
       " (27814, 1),\n",
       " (7293, 1),\n",
       " (15724, 1),\n",
       " (61067, 1),\n",
       " (63385, 1),\n",
       " (43, 1),\n",
       " (1305, 1),\n",
       " (32561, 1),\n",
       " (43211, 1),\n",
       " (87373, 1),\n",
       " (123821, 1),\n",
       " (46639, 1),\n",
       " (89825, 1),\n",
       " (32641, 1),\n",
       " (7457, 1),\n",
       " (14235, 1),\n",
       " (95171, 1),\n",
       " (98377, 1),\n",
       " (42398, 1),\n",
       " (96129, 1),\n",
       " (61749, 1),\n",
       " (85103, 1),\n",
       " (25303, 1),\n",
       " (73256, 1),\n",
       " (62783, 1),\n",
       " (76924, 1),\n",
       " (35079, 1),\n",
       " (13820, 1),\n",
       " (30081, 1),\n",
       " (48414, 1),\n",
       " (8538, 1),\n",
       " (16856, 1),\n",
       " (77850, 1),\n",
       " (47048, 1),\n",
       " (62919, 1),\n",
       " (35919, 1),\n",
       " (23662, 1),\n",
       " (50742, 1),\n",
       " (36966, 1),\n",
       " (38, 1),\n",
       " (60860, 1),\n",
       " (27987, 1),\n",
       " (78286, 1),\n",
       " (9528, 1),\n",
       " (10544, 1),\n",
       " (12978, 1),\n",
       " (50, 1),\n",
       " (51, 1),\n",
       " (9052, 1),\n",
       " (50872, 1),\n",
       " (27014, 1),\n",
       " (58524, 1),\n",
       " (97014, 1),\n",
       " (26004, 1),\n",
       " (51757, 1),\n",
       " (19410, 1),\n",
       " (26509, 1),\n",
       " (80196, 1),\n",
       " (15881, 1),\n",
       " (69112, 1),\n",
       " (25797, 1),\n",
       " (76094, 1),\n",
       " (44211, 1),\n",
       " (56753, 1),\n",
       " (46674, 1),\n",
       " (6777, 1),\n",
       " (15772, 1),\n",
       " (56436, 1),\n",
       " (7778, 1),\n",
       " (70087, 1),\n",
       " (80659, 1)]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sort count dict\n",
    "sc = sorted(count.items(), key=lambda x: -x[1])\n",
    "sc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
