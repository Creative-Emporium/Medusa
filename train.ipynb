{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/camus/work/Medusa/env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaConfig {\n",
       "  \"_name_or_path\": \"/home/camus/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B/snapshots/62bd457b6fe961a42a631306577e622c83876cb6\",\n",
       "  \"architectures\": [\n",
       "    \"LlamaForCausalLM\"\n",
       "  ],\n",
       "  \"attention_bias\": false,\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"bos_token_id\": 128000,\n",
       "  \"eos_token_id\": 128001,\n",
       "  \"hidden_act\": \"silu\",\n",
       "  \"hidden_size\": 4096,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 14336,\n",
       "  \"max_position_embeddings\": 8192,\n",
       "  \"mlp_bias\": false,\n",
       "  \"model_type\": \"llama\",\n",
       "  \"num_attention_heads\": 32,\n",
       "  \"num_hidden_layers\": 32,\n",
       "  \"num_key_value_heads\": 8,\n",
       "  \"pretraining_tp\": 1,\n",
       "  \"rms_norm_eps\": 1e-05,\n",
       "  \"rope_scaling\": null,\n",
       "  \"rope_theta\": 500000.0,\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"torch_dtype\": \"bfloat16\",\n",
       "  \"transformers_version\": \"4.41.2\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 128256\n",
       "}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import transformers\n",
    "\n",
    "from medusa.train.train_legacy import ModelArguments, DataArguments, TrainingArguments\n",
    "\n",
    "\n",
    "model_args = ModelArguments(\n",
    "\tmodel_name_or_path=os.path.expanduser('~/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B/snapshots/62bd457b6fe961a42a631306577e622c83876cb6'),\n",
    ")\n",
    "\n",
    "data_args = DataArguments(\n",
    "\tdata_path=os.path.expanduser('~/data/ShareGPT_Vicuna_unfiltered/shareGPT-llama3-8B.json'),\n",
    "\tlazy_preprocess=True,\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "\toutput_dir='./train/test',\n",
    "\tmedusa_num_heads=5,\n",
    "\tmedusa_num_layers=1,\n",
    ")\n",
    "\n",
    "config = transformers.AutoConfig.from_pretrained(\n",
    "\tmodel_args.model_name_or_path,\n",
    "\tcache_dir=training_args.cache_dir,\n",
    ")\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8192, 2048)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_ctx_len = getattr(config, \"max_position_embeddings\", None)\n",
    "orig_ctx_len, training_args.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None, '<|end_of_text|>')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_args.model_name_or_path,\n",
    "    cache_dir=training_args.cache_dir,\n",
    "    model_max_length=training_args.model_max_length,\n",
    "    padding_side=\"right\",\n",
    "    use_fast=True,\n",
    ")\n",
    "\n",
    "tokenizer.pad_token, tokenizer.unk_token, tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[128000, 2028, 374, 264, 1296], [128000, 19217, 128001, 128001, 128001]], 'attention_mask': [[1, 1, 1, 1, 1], [1, 1, 0, 0, 0]]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer([\"This is a test\", \"secondary\"], padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[128000,\n",
       " 128006,\n",
       " 882,\n",
       " 128007,\n",
       " 271,\n",
       " 2028,\n",
       " 374,\n",
       " 264,\n",
       " 1296,\n",
       " 128009,\n",
       " 128006,\n",
       " 78191,\n",
       " 128007,\n",
       " 271]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.apply_chat_template([{\"role\": \"user\", \"content\": \"This is a test\"}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.98it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "model = transformers.LlamaForCausalLM.from_pretrained(\n",
    "    model_args.model_name_or_path,\n",
    "    config=config,\n",
    "    cache_dir=training_args.cache_dir,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MedusaModel(\n",
       "  (base_model): LlamaForCausalLM(\n",
       "    (model): LlamaModel(\n",
       "      (embed_tokens): Embedding(128256, 4096)\n",
       "      (layers): ModuleList(\n",
       "        (0-31): 32 x LlamaDecoderLayer(\n",
       "          (self_attn): LlamaSdpaAttention(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (rotary_emb): LlamaRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): LlamaMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): LlamaRMSNorm()\n",
       "          (post_attention_layernorm): LlamaRMSNorm()\n",
       "        )\n",
       "      )\n",
       "      (norm): LlamaRMSNorm()\n",
       "    )\n",
       "    (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       "  )\n",
       "  (medusa_head): ModuleList(\n",
       "    (0-4): 5 x Sequential(\n",
       "      (0): ResBlock(\n",
       "        (linear): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "        (act): SiLU()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from medusa.model.medusa_model_legacy import MedusaModel\n",
    "\n",
    "\n",
    "medusa_lm_head = MedusaModel(\n",
    "    model,\n",
    "    medusa_num_heads=training_args.medusa_num_heads,\n",
    "    medusa_num_layers=training_args.medusa_num_layers,\n",
    "    base_model_name_or_path=model_args.model_name_or_path,\n",
    ")\n",
    "medusa_lm_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_dataset': <medusa.train.train_legacy.LazySupervisedDataset at 0x78f5f885bfa0>,\n",
       " 'eval_dataset': None}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from medusa.train.train_legacy import make_supervised_data_module\n",
    "\n",
    "\n",
    "data_module = make_supervised_data_module(tokenizer=tokenizer, data_args=data_args)\n",
    "data_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x78f1a7450c10>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "from transformers.trainer_utils import seed_worker\n",
    "from transformers.data.data_collator import DataCollatorWithPadding\n",
    "\n",
    "\n",
    "default_collator = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "dataloader_params = {\n",
    "\t\"batch_size\": 1,\n",
    "\t\"collate_fn\": default_collator,\n",
    "\t\"num_workers\": 1,\n",
    "\t\"pin_memory\": training_args.dataloader_pin_memory,\n",
    "\t\"persistent_workers\": training_args.dataloader_persistent_workers,\n",
    "\t\"sampler\": RandomSampler(data_module['train_dataset']),\n",
    "\t\"drop_last\": training_args.dataloader_drop_last,\n",
    "\t\"worker_init_fn\": seed_worker,\n",
    "\t\"prefetch_factor\": training_args.dataloader_prefetch_factor,\n",
    "}\n",
    "\n",
    "loader = DataLoader(data_module['train_dataset'], **dataloader_params)\n",
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "it = iter(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[128000, 128000, 128006,  ...,   2763,    315,   8149]]), 'labels': tensor([[  -100, 128000, 128006,  ...,   2763,    315,   8149]]), 'attention_mask': tensor([[True, True, True,  ..., True, True, True]])}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(it)\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 2048]), torch.Size([1, 2048]), torch.Size([1, 2048]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['input_ids'].shape, batch['labels'].shape, batch['attention_mask'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "I am going to write a choose your own adventure role playing book using some mechanics from D&D 5th edition with several homebrew changes, class, and settings. The adventure will take place on a Aztec themed temple called the Yoselotl Temple and I'm initially naming the adventure book, \"Defeat the Dungeon: Temple of Lost Souls.\" First thing is first, I want to go over the lore and history of the world before we get into the hooks/goals for the character going into the dungeon.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "What an exciting project! I'd be happy to help you develop the lore and history of the world for \"Defeat the Dungeon: Temple of Lost Souls.\"\n",
      "\n",
      "Let's start by creating a rich and immersive world. Here's a possible backstory for the Yoselotl Temple and the world it's set in:\n",
      "\n",
      "**World Setting:**\n",
      "The world is called Tenochtitlan, a land of ancient civilizations, mystical energies, and hidden secrets. Tenochtitlan was once a thriving empire, with powerful city-states and advanced civilizations that rivaled the greatest empires of old. However, as the centuries passed, the empire crumbled, and the cities were left to decay. The once-great metropolises became overgrown with jungle, and the remnants of the empire's people were scattered across the land.\n",
      "\n",
      "**The Yoselotl Temple:**\n",
      "The Yoselotl Temple, also known as the Temple of Lost Souls, is an ancient structure built by the Aztec civilization. It was said to be the seat of the divine, where the gods themselves walked among mortals. The temple was constructed around a massive crystal, known as the Heart of Tenochtitlan, which was believed to hold the power to communicate with the gods.\n",
      "\n",
      "According to legend, the temple was built by the Aztecs as a shrine to the gods, and it was said that anyone who possessed the heart could communicate with the divine. Over time, the temple fell into disrepair, and the Heart of Tenochtitlan was lost. The temple became a place of dark legend, rumored to be haunted by the restless spirits of the ancient Aztecs.\n",
      "\n",
      "**The Current State of Tenochtitlan:**\n",
      "Today, Tenochtitlan is a land of mystery and wonder. The jungle has reclaimed much of the ancient city, and the ruins are treacherous and overgrown. The people of Tenochtitlan live in small villages and towns, eking out a living in the shadow of the great temples.\n",
      "\n",
      "Rumors have begun to circulate about strange occurrences in the temple, including visions, whispers in the darkness, and sudden appearances of ghostly apparitions. Many believe that the temple is awakening, and that the Heart of Tenochtitlan is calling out to those who would dare to claim it.\n",
      "\n",
      "**The Cult of the Red Hand:**\n",
      "A mysterious cult, known as the Cult of the Red Hand, has emerged in Tenochtitlan. They are said to be seeking the Heart of Tenochtitlan, and will stop at nothing to claim it. The cult is rumored to be led by a charismatic figure known only as the Señor de la Mano Roja, who is said to possess dark powers and a ruthless determination.\n",
      "\n",
      "The Cult of the Red Hand is said to be hiding in the shadows, waiting for the perfect moment to strike. They are rumored to be recruiting followers from the villages and towns, promising them power and immortality in exchange for their loyalty.\n",
      "\n",
      "**The Hooks:**\n",
      "Now that we have the lore and history of the world, we can start to create the hooks that will draw your players into the adventure. Here are a few possibilities:\n",
      "\n",
      "* A local village is plagued by strange occurrences and unexplained events. The villagers believe that the temple is awakening, and that the Heart of Tenochtitlan is calling out to them.\n",
      "* A group of treasure hunters is seeking the lost Heart of Tenochtitlan, and they need brave adventurers to help them navigate the treacherous jungle and the deadly temple.\n",
      "* A mysterious figure, claiming to be a member of the Cult of the Red Hand, approaches the player characters and offers them a deal: help the cult claim the Heart of Tenochtitlan, and they will be granted incredible power and wealth.\n",
      "\n",
      "These are just a few examples, but there are many other hooks that could be used to draw the players into the adventure. Do any of these resonate with you, or do you have other ideas for the hooks?\n",
      "\n",
      "Next, we can start discussing the mechanics and classes for the adventure. Would you like to use the standard D&D 5th edition classes, or do you have a different setup in mind?<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "I've already made up a lore of the world based on a previous conversation. Here's what I've got so far.\n",
      "\n",
      " I'm thinking of this as the current era being like the 1600s in the real world. The history that I have planned is that a plague hit the whole world around 300 to 500 years ago (I've not decided on a time frame yet so this could change) that did a number on the population, as I've mentioned earlier. \n",
      "\n",
      "Things were a lot different. It would've been your traditional fantasy world. Wizards, barbarians, knights, all sorts of fantasy magic and tropes. You get the idea. This plague, which I'm also thinking it should be a magical plague, but more or less mysterious, starts forming and it would cause that era to vanish essentially. People who were responsible for the world being a fantastic fantasy world dies off. Essentially this is my excuse as to why the world changed from knights, sorcery, and fantasy, to a world that's more grounded by comparison with much fewer people left. \n",
      "\n",
      "When the plague disappeared, people migrated to find new resources to survive, since the previous resource, once provided by an abundance of sorcery, has pretty much vanished. That's how the new world was discovered, a desperate attempt to find something, anything, to replace what the people have lost.\n",
      "\n",
      "The only people who are remotely capable of magic now are witches, who's only wonderous ability is their lightning attack. Aside from that, their magic basically involves enhancing things that people could already do, or enhancing mundane things, all through rituals. Like staying warm through tucking a cold piece of coal wrapped in parchment in their vest, making sure the next arrow they shoot lands in its intended target, or as simple as doing things better. Pretty much how real life witchcraft works I guess.\n",
      "\n",
      "People are now interested in exploration of dungeons of both the old world and new world for the purpose of treasure or other things that will help the world to survive, or as an attempt to bring back the glory of an age long gone. That's all I have thought up of so far.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "I love where this is going! You've got a rich and intriguing world with a compelling history. The idea of a magical plague that wiped out the old world and its magical wonders, leaving behind a more mundane and resource-scarce world, is a great concept.\n",
      "\n",
      "I particularly like how you've described the witches in your world. The idea that they're the only ones with magical abilities, but that those abilities are largely focused on enhancing everyday activities, is a great twist on traditional fantasy magic. It adds a sense of realism and practicality to the world, and makes the concept of magic feel more grounded and accessible.\n",
      "\n",
      "The idea of people exploring dungeons and seeking treasure or resources to help the world survive is a great direction to take the story. It opens up opportunities for all sorts of adventures and conflicts, and allows you to explore themes of survival, discovery, and the human condition.\n",
      "\n",
      "Now that we have a better sense of the world and its history, let's talk about the Yoselotl Temple and the adventure itself. What kind of story do you envision for the players? Are they seeking treasure, trying to uncover the secrets of the temple, or something else entirely?\n",
      "\n",
      "Also, have you thought about the tone of the adventure? Do you want it to be light-hearted and humorous, or more serious and dark? And what kind of NPCs do you see popping up throughout the adventure?<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "I've already got the hook. The character is going into the temple to retrieve the Golden Mask of Yoselotl to pay off their debt. I am in the process of writing multiple organizations and groups that the player can choose who the character is in debt to.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "So the hook is that the character is going into the temple to retrieve the Golden Mask of Yoselotl to pay off a debt to one of the organizations you've created. That's a great hook because it gives the player a clear goal and motivation to drive the story forward.\n",
      "\n",
      "I'm curious, what kind of organizations have you created? Are they all vying for power, or are they more focused on survival and stability in this new world? And what kind of debt is the character in? Is it a loan, a favor, or something else?\n",
      "\n",
      "Also, have you thought about the challenges and obstacles that the character will face in the temple? Will it be magical traps, puzzles, or physical challenges? What kind of enemies will they encounter, and what kind of treasures and secrets will they find along the way?\n",
      "\n",
      "Lastly, what's the significance of the Golden Mask of Yoselotl? Is it a powerful artifact, a symbol of prestige, or something else entirely? How will the character's retrieval of the mask affect the world and the organizations they owe a debt to?<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "I should also point out that the player character will be a Witch class that I've homebrewed with various customizable options, some reflecting features of official D&D classes.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "That sounds like a lot of fun! A homebrewed Witch class with customizable options can add a lot of depth\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(batch['input_ids'][0], skip_special_tokens=False, spaces_between_special_tokens=False, clean_up_tokenization_spaces=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[-100,\n",
       "  128000,\n",
       "  128006,\n",
       "  -100,\n",
       "  128007,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  2162,\n",
       "  22363,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  15757,\n",
       "  55300,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  816,\n",
       "  437,\n",
       "  301,\n",
       "  354,\n",
       "  75,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  2685,\n",
       "  33166,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  26606,\n",
       "  1147,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  128009,\n",
       "  128006,\n",
       "  -100,\n",
       "  128007,\n",
       "  -100,\n",
       "  3923,\n",
       "  459,\n",
       "  13548,\n",
       "  2447,\n",
       "  0,\n",
       "  358,\n",
       "  4265,\n",
       "  387,\n",
       "  6380,\n",
       "  311,\n",
       "  1520,\n",
       "  499,\n",
       "  2274,\n",
       "  279,\n",
       "  52322,\n",
       "  323,\n",
       "  3925,\n",
       "  315,\n",
       "  279,\n",
       "  1917,\n",
       "  369,\n",
       "  330,\n",
       "  2685,\n",
       "  33166,\n",
       "  279,\n",
       "  51947,\n",
       "  25,\n",
       "  19176,\n",
       "  315,\n",
       "  28351,\n",
       "  58770,\n",
       "  2266,\n",
       "  10267,\n",
       "  596,\n",
       "  1212,\n",
       "  555,\n",
       "  6968,\n",
       "  264,\n",
       "  9257,\n",
       "  323,\n",
       "  65280,\n",
       "  1917,\n",
       "  13,\n",
       "  5810,\n",
       "  596,\n",
       "  264,\n",
       "  3284,\n",
       "  93371,\n",
       "  369,\n",
       "  279,\n",
       "  816,\n",
       "  437,\n",
       "  301,\n",
       "  354,\n",
       "  75,\n",
       "  19176,\n",
       "  323,\n",
       "  279,\n",
       "  1917,\n",
       "  433,\n",
       "  596,\n",
       "  743,\n",
       "  304,\n",
       "  1473,\n",
       "  334,\n",
       "  10343,\n",
       "  20638,\n",
       "  25,\n",
       "  1035,\n",
       "  791,\n",
       "  1917,\n",
       "  374,\n",
       "  2663,\n",
       "  18165,\n",
       "  5059,\n",
       "  17330,\n",
       "  10946,\n",
       "  11,\n",
       "  264,\n",
       "  4363,\n",
       "  315,\n",
       "  14154,\n",
       "  86569,\n",
       "  11,\n",
       "  69532,\n",
       "  49969,\n",
       "  11,\n",
       "  323,\n",
       "  8340,\n",
       "  24511,\n",
       "  13,\n",
       "  18165,\n",
       "  5059,\n",
       "  17330,\n",
       "  10946,\n",
       "  574,\n",
       "  3131,\n",
       "  264,\n",
       "  53414,\n",
       "  32447,\n",
       "  11,\n",
       "  449,\n",
       "  8147,\n",
       "  3363,\n",
       "  90160,\n",
       "  323,\n",
       "  11084,\n",
       "  86569,\n",
       "  430,\n",
       "  44229,\n",
       "  5962,\n",
       "  279,\n",
       "  12474,\n",
       "  991,\n",
       "  19505,\n",
       "  315,\n",
       "  2362,\n",
       "  13,\n",
       "  4452,\n",
       "  11,\n",
       "  439,\n",
       "  279,\n",
       "  24552,\n",
       "  5946,\n",
       "  11,\n",
       "  279,\n",
       "  32447,\n",
       "  1589,\n",
       "  26902,\n",
       "  11,\n",
       "  323,\n",
       "  279,\n",
       "  9919,\n",
       "  1051,\n",
       "  2163,\n",
       "  311,\n",
       "  31815,\n",
       "  13,\n",
       "  578,\n",
       "  3131,\n",
       "  2427,\n",
       "  1244,\n",
       "  2322,\n",
       "  23143,\n",
       "  5014,\n",
       "  6244,\n",
       "  927,\n",
       "  62376,\n",
       "  449,\n",
       "  45520,\n",
       "  11,\n",
       "  323,\n",
       "  279,\n",
       "  73440,\n",
       "  315,\n",
       "  279,\n",
       "  32447,\n",
       "  596,\n",
       "  1274,\n",
       "  1051,\n",
       "  38067,\n",
       "  4028,\n",
       "  279,\n",
       "  4363,\n",
       "  382,\n",
       "  334,\n",
       "  791,\n",
       "  816,\n",
       "  437,\n",
       "  301,\n",
       "  354,\n",
       "  75,\n",
       "  19176,\n",
       "  25,\n",
       "  1035,\n",
       "  791,\n",
       "  816,\n",
       "  437,\n",
       "  301,\n",
       "  354,\n",
       "  75,\n",
       "  19176,\n",
       "  11,\n",
       "  1101,\n",
       "  3967,\n",
       "  439,\n",
       "  279,\n",
       "  19176,\n",
       "  315,\n",
       "  28351,\n",
       "  58770,\n",
       "  11,\n",
       "  374,\n",
       "  459,\n",
       "  14154,\n",
       "  6070,\n",
       "  5918,\n",
       "  555,\n",
       "  279,\n",
       "  15757,\n",
       "  55300,\n",
       "  36017,\n",
       "  13,\n",
       "  1102,\n",
       "  574,\n",
       "  1071,\n",
       "  311,\n",
       "  387,\n",
       "  279,\n",
       "  10954,\n",
       "  315,\n",
       "  279,\n",
       "  30467,\n",
       "  11,\n",
       "  1405,\n",
       "  279,\n",
       "  29913,\n",
       "  5694,\n",
       "  15203,\n",
       "  4315,\n",
       "  10237,\n",
       "  1147,\n",
       "  13,\n",
       "  578,\n",
       "  27850,\n",
       "  574,\n",
       "  20968,\n",
       "  2212,\n",
       "  264,\n",
       "  11191,\n",
       "  26110,\n",
       "  11,\n",
       "  3967,\n",
       "  439,\n",
       "  279,\n",
       "  18449,\n",
       "  315,\n",
       "  18165,\n",
       "  5059,\n",
       "  17330,\n",
       "  10946,\n",
       "  11,\n",
       "  902,\n",
       "  574,\n",
       "  11846,\n",
       "  311,\n",
       "  3412,\n",
       "  279,\n",
       "  2410,\n",
       "  311,\n",
       "  19570,\n",
       "  449,\n",
       "  279,\n",
       "  29913,\n",
       "  382,\n",
       "  11439,\n",
       "  311,\n",
       "  13314,\n",
       "  11,\n",
       "  279,\n",
       "  27850,\n",
       "  574,\n",
       "  5918,\n",
       "  555,\n",
       "  279,\n",
       "  15757,\n",
       "  668,\n",
       "  4942,\n",
       "  439,\n",
       "  264,\n",
       "  74648,\n",
       "  311,\n",
       "  279,\n",
       "  29913,\n",
       "  11,\n",
       "  323,\n",
       "  433,\n",
       "  574,\n",
       "  1071,\n",
       "  430,\n",
       "  5606,\n",
       "  889,\n",
       "  43890,\n",
       "  279,\n",
       "  4851,\n",
       "  1436,\n",
       "  19570,\n",
       "  449,\n",
       "  279,\n",
       "  30467,\n",
       "  13,\n",
       "  6193,\n",
       "  892,\n",
       "  11,\n",
       "  279,\n",
       "  27850,\n",
       "  11299,\n",
       "  1139,\n",
       "  834,\n",
       "  77539,\n",
       "  11,\n",
       "  323,\n",
       "  279,\n",
       "  18449,\n",
       "  315,\n",
       "  18165,\n",
       "  5059,\n",
       "  17330,\n",
       "  10946,\n",
       "  574,\n",
       "  5675,\n",
       "  13,\n",
       "  578,\n",
       "  27850,\n",
       "  6244,\n",
       "  264,\n",
       "  2035,\n",
       "  315,\n",
       "  6453,\n",
       "  13314,\n",
       "  11,\n",
       "  70468,\n",
       "  311,\n",
       "  387,\n",
       "  57775,\n",
       "  555,\n",
       "  279,\n",
       "  89358,\n",
       "  31739,\n",
       "  315,\n",
       "  279,\n",
       "  14154,\n",
       "  15757,\n",
       "  668,\n",
       "  4942,\n",
       "  382,\n",
       "  334,\n",
       "  791,\n",
       "  9303,\n",
       "  3314,\n",
       "  315,\n",
       "  18165,\n",
       "  5059,\n",
       "  17330,\n",
       "  10946,\n",
       "  25,\n",
       "  1035,\n",
       "  15724,\n",
       "  11,\n",
       "  18165,\n",
       "  5059,\n",
       "  17330,\n",
       "  10946,\n",
       "  374,\n",
       "  264,\n",
       "  4363,\n",
       "  315,\n",
       "  23347,\n",
       "  323,\n",
       "  5895,\n",
       "  13,\n",
       "  578,\n",
       "  45520,\n",
       "  706,\n",
       "  89759,\n",
       "  1790,\n",
       "  315,\n",
       "  279,\n",
       "  14154,\n",
       "  3363,\n",
       "  11,\n",
       "  323,\n",
       "  279,\n",
       "  46762,\n",
       "  527,\n",
       "  4353,\n",
       "  11252,\n",
       "  788,\n",
       "  323,\n",
       "  927,\n",
       "  62376,\n",
       "  13,\n",
       "  578,\n",
       "  1274,\n",
       "  315,\n",
       "  18165,\n",
       "  5059,\n",
       "  17330,\n",
       "  10946,\n",
       "  3974,\n",
       "  304,\n",
       "  2678,\n",
       "  33889,\n",
       "  323,\n",
       "  25861,\n",
       "  11,\n",
       "  384,\n",
       "  10789,\n",
       "  704,\n",
       "  264,\n",
       "  5496,\n",
       "  304,\n",
       "  279,\n",
       "  12737,\n",
       "  315,\n",
       "  279,\n",
       "  2294,\n",
       "  57997,\n",
       "  382,\n",
       "  49,\n",
       "  372,\n",
       "  1105,\n",
       "  617,\n",
       "  22088,\n",
       "  311,\n",
       "  4319,\n",
       "  6468,\n",
       "  922,\n",
       "  15234,\n",
       "  57115,\n",
       "  304,\n",
       "  279,\n",
       "  27850,\n",
       "  11,\n",
       "  2737,\n",
       "  58395,\n",
       "  11,\n",
       "  89248,\n",
       "  304,\n",
       "  279,\n",
       "  27394,\n",
       "  11,\n",
       "  323,\n",
       "  11210,\n",
       "  27351,\n",
       "  315,\n",
       "  20457,\n",
       "  398,\n",
       "  34388,\n",
       "  6055,\n",
       "  13,\n",
       "  9176,\n",
       "  4510,\n",
       "  430,\n",
       "  279,\n",
       "  27850,\n",
       "  374,\n",
       "  75149,\n",
       "  11,\n",
       "  323,\n",
       "  430,\n",
       "  279,\n",
       "  18449,\n",
       "  315,\n",
       "  18165,\n",
       "  5059,\n",
       "  17330,\n",
       "  10946,\n",
       "  374,\n",
       "  8260,\n",
       "  704,\n",
       "  311,\n",
       "  1884,\n",
       "  889,\n",
       "  1053,\n",
       "  33304,\n",
       "  311,\n",
       "  3802,\n",
       "  433,\n",
       "  382,\n",
       "  334,\n",
       "  791,\n",
       "  26676,\n",
       "  315,\n",
       "  279,\n",
       "  3816,\n",
       "  8704,\n",
       "  25,\n",
       "  1035,\n",
       "  32,\n",
       "  26454,\n",
       "  4612,\n",
       "  11,\n",
       "  3967,\n",
       "  439,\n",
       "  279,\n",
       "  26676,\n",
       "  315,\n",
       "  279,\n",
       "  3816,\n",
       "  8704,\n",
       "  11,\n",
       "  706,\n",
       "  22763,\n",
       "  304,\n",
       "  18165,\n",
       "  5059,\n",
       "  17330,\n",
       "  10946,\n",
       "  13,\n",
       "  2435,\n",
       "  527,\n",
       "  1071,\n",
       "  311,\n",
       "  387,\n",
       "  11125,\n",
       "  279,\n",
       "  18449,\n",
       "  315,\n",
       "  18165,\n",
       "  5059,\n",
       "  17330,\n",
       "  10946,\n",
       "  11,\n",
       "  323,\n",
       "  690,\n",
       "  3009,\n",
       "  520,\n",
       "  4400,\n",
       "  311,\n",
       "  3802,\n",
       "  433,\n",
       "  13,\n",
       "  578,\n",
       "  4612,\n",
       "  374,\n",
       "  70468,\n",
       "  311,\n",
       "  387,\n",
       "  6197,\n",
       "  555,\n",
       "  264,\n",
       "  79323,\n",
       "  7216,\n",
       "  3967,\n",
       "  1193,\n",
       "  439,\n",
       "  279,\n",
       "  1369,\n",
       "  5771,\n",
       "  269,\n",
       "  409,\n",
       "  1208,\n",
       "  2418,\n",
       "  78,\n",
       "  12093,\n",
       "  5697,\n",
       "  11,\n",
       "  889,\n",
       "  374,\n",
       "  1071,\n",
       "  311,\n",
       "  15575,\n",
       "  6453,\n",
       "  13736,\n",
       "  323,\n",
       "  264,\n",
       "  69510,\n",
       "  26314,\n",
       "  382,\n",
       "  791,\n",
       "  26676,\n",
       "  315,\n",
       "  279,\n",
       "  3816,\n",
       "  8704,\n",
       "  374,\n",
       "  1071,\n",
       "  311,\n",
       "  387,\n",
       "  26092,\n",
       "  304,\n",
       "  279,\n",
       "  35612,\n",
       "  11,\n",
       "  8748,\n",
       "  369,\n",
       "  279,\n",
       "  4832,\n",
       "  4545,\n",
       "  311,\n",
       "  13471,\n",
       "  13,\n",
       "  2435,\n",
       "  527,\n",
       "  70468,\n",
       "  311,\n",
       "  387,\n",
       "  32959,\n",
       "  20723,\n",
       "  505,\n",
       "  279,\n",
       "  33889,\n",
       "  323,\n",
       "  25861,\n",
       "  11,\n",
       "  26455,\n",
       "  1124,\n",
       "  2410,\n",
       "  323,\n",
       "  4998,\n",
       "  76052,\n",
       "  304,\n",
       "  9473,\n",
       "  369,\n",
       "  872,\n",
       "  32883,\n",
       "  382,\n",
       "  334,\n",
       "  791,\n",
       "  39530,\n",
       "  25,\n",
       "  1035,\n",
       "  7184,\n",
       "  430,\n",
       "  584,\n",
       "  617,\n",
       "  279,\n",
       "  52322,\n",
       "  323,\n",
       "  3925,\n",
       "  315,\n",
       "  279,\n",
       "  1917,\n",
       "  11,\n",
       "  584,\n",
       "  649,\n",
       "  1212,\n",
       "  311,\n",
       "  1893,\n",
       "  279,\n",
       "  30777,\n",
       "  430,\n",
       "  690,\n",
       "  4128,\n",
       "  701,\n",
       "  4311,\n",
       "  1139,\n",
       "  279,\n",
       "  18427,\n",
       "  13,\n",
       "  5810,\n",
       "  527,\n",
       "  264,\n",
       "  2478,\n",
       "  24525,\n",
       "  1473,\n",
       "  9,\n",
       "  362,\n",
       "  2254,\n",
       "  14458,\n",
       "  374,\n",
       "  66402,\n",
       "  555,\n",
       "  15234,\n",
       "  57115,\n",
       "  323,\n",
       "  653,\n",
       "  81950,\n",
       "  4455,\n",
       "  13,\n",
       "  578,\n",
       "  61921,\n",
       "  4510,\n",
       "  430,\n",
       "  279,\n",
       "  27850,\n",
       "  374,\n",
       "  75149,\n",
       "  11,\n",
       "  323,\n",
       "  430,\n",
       "  279,\n",
       "  18449,\n",
       "  315,\n",
       "  18165,\n",
       "  5059,\n",
       "  17330,\n",
       "  10946,\n",
       "  374,\n",
       "  8260,\n",
       "  704,\n",
       "  311,\n",
       "  1124,\n",
       "  627,\n",
       "  9,\n",
       "  362,\n",
       "  1912,\n",
       "  315,\n",
       "  32726,\n",
       "  49073,\n",
       "  374,\n",
       "  11125,\n",
       "  279,\n",
       "  5675,\n",
       "  18449,\n",
       "  315,\n",
       "  18165,\n",
       "  5059,\n",
       "  17330,\n",
       "  10946,\n",
       "  11,\n",
       "  323,\n",
       "  814,\n",
       "  1205,\n",
       "  34300,\n",
       "  88813,\n",
       "  311,\n",
       "  1520,\n",
       "  1124,\n",
       "  21546,\n",
       "  279,\n",
       "  4353,\n",
       "  11252,\n",
       "  788,\n",
       "  45520,\n",
       "  323,\n",
       "  279,\n",
       "  25114,\n",
       "  27850,\n",
       "  627,\n",
       "  9,\n",
       "  362,\n",
       "  26454,\n",
       "  7216,\n",
       "  11,\n",
       "  21039,\n",
       "  311,\n",
       "  387,\n",
       "  264,\n",
       "  4562,\n",
       "  315,\n",
       "  279,\n",
       "  26676,\n",
       "  315,\n",
       "  279,\n",
       "  3816,\n",
       "  8704,\n",
       "  11,\n",
       "  20414,\n",
       "  279,\n",
       "  2851,\n",
       "  5885,\n",
       "  323,\n",
       "  6209,\n",
       "  1124,\n",
       "  264,\n",
       "  3568,\n",
       "  25,\n",
       "  1520,\n",
       "  279,\n",
       "  4612,\n",
       "  3802,\n",
       "  279,\n",
       "  18449,\n",
       "  315,\n",
       "  18165,\n",
       "  5059,\n",
       "  17330,\n",
       "  10946,\n",
       "  11,\n",
       "  323,\n",
       "  814,\n",
       "  690,\n",
       "  387,\n",
       "  11938,\n",
       "  15400,\n",
       "  2410,\n",
       "  323,\n",
       "  12205,\n",
       "  382,\n",
       "  9673,\n",
       "  527,\n",
       "  1120,\n",
       "  264,\n",
       "  2478,\n",
       "  10507,\n",
       "  11,\n",
       "  719,\n",
       "  1070,\n",
       "  527,\n",
       "  1690,\n",
       "  1023,\n",
       "  30777,\n",
       "  430,\n",
       "  1436,\n",
       "  387,\n",
       "  1511,\n",
       "  311,\n",
       "  4128,\n",
       "  279,\n",
       "  4311,\n",
       "  1139,\n",
       "  279,\n",
       "  18427,\n",
       "  13,\n",
       "  3234,\n",
       "  904,\n",
       "  315,\n",
       "  1521,\n",
       "  89986,\n",
       "  449,\n",
       "  499,\n",
       "  11,\n",
       "  477,\n",
       "  656,\n",
       "  499,\n",
       "  617,\n",
       "  1023,\n",
       "  6848,\n",
       "  369,\n",
       "  279,\n",
       "  30777,\n",
       "  1980,\n",
       "  5971,\n",
       "  11,\n",
       "  584,\n",
       "  649,\n",
       "  1212,\n",
       "  25394,\n",
       "  279,\n",
       "  30126,\n",
       "  323,\n",
       "  6989,\n",
       "  369,\n",
       "  279,\n",
       "  18427,\n",
       "  13,\n",
       "  19418,\n",
       "  499,\n",
       "  1093,\n",
       "  311,\n",
       "  1005,\n",
       "  279,\n",
       "  5410,\n",
       "  423,\n",
       "  33465,\n",
       "  220,\n",
       "  20,\n",
       "  339,\n",
       "  14002,\n",
       "  6989,\n",
       "  11,\n",
       "  477,\n",
       "  656,\n",
       "  499,\n",
       "  617,\n",
       "  264,\n",
       "  2204,\n",
       "  6642,\n",
       "  304,\n",
       "  4059,\n",
       "  30,\n",
       "  128009,\n",
       "  128006,\n",
       "  882,\n",
       "  128007,\n",
       "  271,\n",
       "  40,\n",
       "  3077,\n",
       "  2736,\n",
       "  1903,\n",
       "  709,\n",
       "  264,\n",
       "  52322,\n",
       "  315,\n",
       "  279,\n",
       "  1917,\n",
       "  3196,\n",
       "  389,\n",
       "  264,\n",
       "  3766,\n",
       "  10652,\n",
       "  13,\n",
       "  5810,\n",
       "  596,\n",
       "  1148,\n",
       "  358,\n",
       "  3077,\n",
       "  2751,\n",
       "  779,\n",
       "  3117,\n",
       "  382,\n",
       "  ...]]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['labels'].tolist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
